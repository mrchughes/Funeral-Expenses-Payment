from datetime import datetime
# ...existing code...

from dotenv import load_dotenv
import os
import logging
import sys
import json
import time
import gc  # Add garbage collection module
from flask import Flask, request, jsonify, render_template, redirect, url_for
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langgraph.graph import StateGraph, END
# from langgraph import State
from langchain_community.tools.tavily_search import TavilySearchResults
from typing_extensions import TypedDict
from langchain_community.vectorstores import Chroma
from werkzeug.utils import secure_filename
# OCR imports
import ocr_utils
import document_processor
import ai_document_processor

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s %(levelname)s %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(os.path.join(os.path.dirname(os.path.abspath(__file__)), "agent.log"))
    ]
)

load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")
tavily_key = os.getenv("TAVILY_API_KEY")


# Flask app setup
# Set template_folder to absolute path for reliability
from flask_cors import CORS
TEMPLATE_DIR = os.path.join(os.path.dirname(__file__), 'templates')
app = Flask(__name__, template_folder=TEMPLATE_DIR)
# Enable CORS for all origins when using Cloudflare
CORS(app, 
     origins=["*"],  # Allow all origins since Cloudflare will handle security
     supports_credentials=True,
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization", "X-Requested-With"])

# Log CORS configuration
cloudflare_url = os.getenv("CLOUDFLARE_URL", "Not set")
frontend_url = os.getenv("FRONTEND_URL", "Not set")
logging.info(f"[CONFIG] CORS enabled with Cloudflare URL: {cloudflare_url}, Frontend URL: {frontend_url}")

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Use shared Docker volume for evidence
# Check if we're in a Docker container (shared-evidence exists) or local development
if os.path.exists('/shared-evidence'):
    app.config['UPLOAD_FOLDER'] = '/shared-evidence'
else:
    # Fallback to a local directory for development
    app.config['UPLOAD_FOLDER'] = os.path.join(os.path.dirname(__file__), 'uploads', 'evidence')
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
logging.info(f"[CONFIG] AI agent evidence folder set to {app.config['UPLOAD_FOLDER']}")

# Use a separate folder for policy documents (RAG knowledge base)
app.config['POLICY_UPLOAD_FOLDER'] = os.path.join(os.path.dirname(__file__), 'policy_docs')
os.makedirs(app.config['POLICY_UPLOAD_FOLDER'], exist_ok=True)
logging.info(f"[CONFIG] AI agent policy folder set to {app.config['POLICY_UPLOAD_FOLDER']}")

# RAG setup
persist_dir = os.path.join(os.path.dirname(__file__), 'chroma_db')
if not openai_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment.")
embeddings = OpenAIEmbeddings(openai_api_key=openai_key)

# Define a function to load or reload the RAG database
def load_rag_database():
    global rag_db
    try:
        if os.path.exists(persist_dir):
            logging.info(f"[INIT] Loading RAG database from {persist_dir}")
            
            # Re-initialize embeddings to ensure they match what was used during ingestion
            local_embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
            rag_db = Chroma(persist_directory=persist_dir, embedding_function=local_embeddings)
            
            # Verify DB has documents
            db_data = rag_db.get()
            if db_data and 'documents' in db_data:
                doc_count = len(db_data['documents'])
                logging.info(f"[INIT] Successfully loaded RAG database with {doc_count} chunks")
            else:
                logging.warning("[INIT] RAG database exists but contains no documents")
            
            return True
        else:
            logging.info("[INIT] No RAG database found at {persist_dir}")
            rag_db = None
            return False
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[INIT] Error loading RAG database: {e}", exc_info=True)
        rag_db = None
        return False

# Initialize RAG database
rag_db = None
load_rag_database()

# Function to force a reload of the RAG database
def force_reload_rag_database():
    global rag_db, embeddings
    
    try:
        logging.info("[RELOAD] Forcing reload of RAG database")
        persist_dir = os.path.join(os.path.dirname(__file__), 'chroma_db')
        
        if not os.path.exists(persist_dir):
            logging.warning(f"[RELOAD] RAG database directory not found at {persist_dir}")
            return False
        
        # Completely close and reset the existing database
        try:
            if rag_db is not None:
                # Try to persist any changes
                try:
                    rag_db.persist()
                    logging.info("[RELOAD] Persisted existing database")
                except Exception as persist_err:
                    logging.error(f"[RELOAD] Error persisting database: {persist_err}")
                
                # Try to explicitly close client connections
                try:
                    if hasattr(rag_db, '_client') and rag_db._client:
                        rag_db._client.close()
                        logging.info("[RELOAD] Closed database client")
                except Exception as close_err:
                    logging.error(f"[RELOAD] Error closing database client: {close_err}")
        except Exception as cleanup_err:
            logging.error(f"[RELOAD] Error during database cleanup: {cleanup_err}")
        
        # Reset to None
        rag_db = None
        
        # Force Python garbage collection to ensure resources are freed
        import gc
        gc.collect()
        
        # Wait a moment to ensure all connections are closed
        import time
        time.sleep(0.5)
        
        # Re-initialize embeddings
        local_embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
        
        # Create a completely new instance
        logging.info("[RELOAD] Creating new Chroma instance")
        rag_db = Chroma(persist_directory=persist_dir, embedding_function=local_embeddings)
        
        # Verify it loaded correctly
        db_data = rag_db.get()
        if db_data and 'documents' in db_data:
            doc_count = len(db_data['documents'])
            logging.info(f"[RELOAD] Successfully reloaded RAG database with {doc_count} chunks")
            return True
        else:
            logging.warning("[RELOAD] Reloaded RAG database has no documents")
            return False
    except Exception as e:
        logging.error(f"[RELOAD] Error reloading RAG database: {e}", exc_info=True)
        return False

# Initialize LLM globally
llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_key) if openai_key else None
if llm:
    logging.info("[INIT] LLM initialized successfully with OpenAI API key")
else:
    logging.error("[INIT] Failed to initialize LLM - missing OpenAI API key")

# 5. Flask route for the UI with background image

from flask import Blueprint, send_from_directory

# Add a test route for CORS debugging
@app.route('/api/test-cors', methods=['GET'])
def test_cors():
    return jsonify({'success': True, 'message': 'CORS is working properly'})

# Serve static files
from docx import Document
import PyPDF2
ai_agent_bp = Blueprint('ai_agent', __name__, url_prefix='/ai-agent')

@ai_agent_bp.route('/extract-form-data', methods=['POST'])
def extract_form_data():
    docs_dir = app.config['UPLOAD_FOLDER']
    logging.info(f"[EXTRACT] Scanning evidence directory: {docs_dir}")
    extracted = {}
    
    # Initialize document processor for OCR
    document_processor_instance = document_processor.DocumentProcessor(upload_folder=docs_dir)
    
    # Initialize AI document processor for langchain integration
    ai_document_processor_instance = None
    try:
        ai_document_processor_instance = ai_document_processor.AIDocumentProcessor()
        logging.info("[EXTRACT] AI Document Processor initialized successfully")
    except Exception as e:
        logging.error(f"[EXTRACT] Failed to initialize AI Document Processor: {e}", exc_info=True)
    
    # Get the list of files from the request, if provided
    requested_files = []
    if request.json and 'files' in request.json:
        requested_files = request.json.get('files', [])
        logging.info(f"[EXTRACT] Processing requested files: {requested_files}")
    
    # Process all files in the directory if no specific files requested
    file_list = requested_files if requested_files else os.listdir(docs_dir)
    
    for fname in file_list:
        file_path = os.path.join(docs_dir, fname)
        if not os.path.exists(file_path):
            logging.warning(f"[EXTRACT] File not found: {file_path}")
            extracted[fname] = "Error: File not found"
            continue
            
        if not os.path.isfile(file_path):
            logging.warning(f"[EXTRACT] Not a file: {file_path}")
            continue
            
        logging.info(f"[EXTRACT] Processing file: {file_path}")
        
        try:
            # Check file extension to determine processing method
            _, ext = os.path.splitext(file_path)
            ext = ext.lower()
            
            # Process image files with OCR
            if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']:
                logging.info(f"[EXTRACT] Processing image file with OCR: {file_path}")
                
                # Process the file with OCR
                ocr_result = document_processor_instance.process_file(file_path)
                
                if not ocr_result.get("success", False):
                    logging.error(f"[EXTRACT] OCR processing failed: {ocr_result.get('error')}")
                    extracted[fname] = f"Error: OCR processing failed: {ocr_result.get('error')}"
                    continue
                    
                content = ocr_result.get("text", "")
                logging.info(f"[EXTRACT] Successfully extracted {len(content)} characters from image")
                
            # Process text, PDF, and DOCX files as before
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            elif ext == '.docx':
                doc = Document(file_path)
                content = '\n'.join([para.text for para in doc.paragraphs])
            elif ext == '.pdf':
                content = ""
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        content += page.extract_text() or ""
            else:
                content = f"File {fname} is not a supported type."
                
            # Application schema summary (field: description)
            schema = '''
firstName: Applicant's first name
lastName: Applicant's last name
dateOfBirth: Applicant's date of birth
nationalInsuranceNumber: Applicant's National Insurance number
addressLine1: Address line 1
addressLine2: Address line 2
town: Town or city
county: County
postcode: Postcode
phoneNumber: Phone number
email: Email address
partnerFirstName: Partner's first name
partnerLastName: Partner's last name
partnerDateOfBirth: Partner's date of birth
partnerNationalInsuranceNumber: Partner's National Insurance number
partnerBenefitsReceived: Benefits the partner receives
partnerSavings: Partner's savings
deceasedFirstName: Deceased's first name
deceasedLastName: Deceased's last name
deceasedDateOfBirth: Deceased's date of birth
deceasedDateOfDeath: Deceased's date of death
deceasedPlaceOfDeath: Place of death
deceasedCauseOfDeath: Cause of death
deceasedCertifyingDoctor: Certifying doctor
deceasedCertificateIssued: Certificate issued
relationshipToDeceased: Relationship to deceased
supportingEvidence: Supporting evidence
responsibilityStatement: Responsibility statement
responsibilityDate: Responsibility date
benefitType: Type of benefit
benefitReferenceNumber: Benefit reference number
benefitLetterDate: Date on benefit letter
householdBenefits: Household benefits (array)
incomeSupportDetails: Details about Income Support
disabilityBenefits: Disability benefits (array)
carersAllowance: Carer's Allowance
carersAllowanceDetails: Carer's Allowance details
funeralDirector: Funeral director
funeralEstimateNumber: Funeral estimate number
funeralDateIssued: Date funeral estimate issued
funeralTotalEstimatedCost: Total estimated funeral cost
funeralDescription: Funeral description
funeralContact: Funeral contact
evidence: Evidence documents (array)
'''
            prompt = f'''
You are an expert assistant helping to process evidence for a funeral expenses claim. The following is the application schema:
{schema}

Read the following evidence and extract all information relevant to the claim. For each field you extract, provide:
- The field name (from the schema above)
- The value
- A short explanation of your reasoning or the evidence source (e.g. "Found in death certificate under 'Date of death'")
If a field is not directly mentioned but can be inferred, include it and explain your inference.
Return your answer as a JSON object where each key is a field name, and each value is an object with 'value' and 'reasoning'.

Evidence:
{content}
'''
            if llm is None:
                logging.error(f"[EXTRACT ERROR] {fname}: LLM not initialized properly")
                extracted[fname] = "Error: AI model not available. Check OpenAI API key configuration."
            else:
                response = llm.invoke(prompt)
                extracted[fname] = str(response.content) if hasattr(response, 'content') else str(response)
                logging.info(f"[EXTRACT] Extraction result for {fname}: {extracted[fname]}")
        except Exception as e:
            logging.error(f"[EXTRACT ERROR] {fname}: {e}", exc_info=True)
            extracted[fname] = f"Error extracting: {e}"
    return jsonify(extracted)

# --- List policy documents in RAG ---
@ai_agent_bp.route('/docs', methods=['GET'])
def list_docs():
    global rag_db
    """List all policy documents in the system with accurate chunk counts"""
    try:
        docs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "policy_docs")
        logging.info(f"[DOCS] Listing documents from {docs_dir}")
        
        # Force reload RAG database if requested or by default
        force_reload = request.args.get('force_reload', 'true').lower() == 'true'  # Changed default to true
        if force_reload:
            logging.info("[DOCS] Force reloading RAG database")
            try:
                # global rag_db (already declared at top)
                script_dir = os.path.dirname(os.path.abspath(__file__))
                persist_dir = os.path.join(script_dir, "chroma_db")
                
                # Fix permissions on database directory before loading
                try:
                    if os.path.exists(persist_dir):
                        os.chmod(persist_dir, 0o777)
                        # Fix permissions on all files and directories
                        for root, dirs, files in os.walk(persist_dir):
                            for d in dirs:
                                try:
                                    os.chmod(os.path.join(root, d), 0o777)
                                except Exception as e:
                                    logging.warning(f"[DOCS] Could not set permissions on directory {d}: {e}")
                            for f in files:
                                try:
                                    os.chmod(os.path.join(root, f), 0o666)
                                except Exception as e:
                                    logging.warning(f"[DOCS] Could not set permissions on file {f}: {e}")
                except Exception as perm_err:
                    logging.warning(f"[DOCS] Permission fix warning (non-fatal): {perm_err}")
                
                # Close and clear the existing database connection
                if rag_db is not None:
                    try:
                        rag_db = None
                        gc.collect()  # Force garbage collection
                    except:
                        pass
                
                # Create fresh embeddings and reload database
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                logging.info("[DOCS] Successfully reloaded RAG database")
            except Exception as reload_err:
                logging.error(f"[DOCS] Error reloading RAG database: {reload_err}", exc_info=True)
        
        files = []
        # Get list of actual files on disk
        for filename in os.listdir(docs_dir):
            if filename.lower().endswith(('.pdf', '.docx', '.txt')):
                file_path = os.path.join(docs_dir, filename)
                if os.path.isfile(file_path):
                    file_info = {
                        'name': filename,
                        'size': os.path.getsize(file_path),
                        'last_modified': os.path.getmtime(file_path),
                        'chunk_count': 0  # Will be updated with actual count below
                    }
                    files.append(file_info)

        # Sort by most recently modified
        files.sort(key=lambda x: x['last_modified'], reverse=True)
        
        # Get accurate per-document chunk counts from the vector database
        # global rag_db  # Not needed here, only reading
        rag_status = {
            'initialized': False, 
            'total_chunk_count': 0,
            'per_document': {},
            'orphaned_chunks': 0
        }
        
        if rag_db is not None:
            try:
                # Force a fresh load of the database to get latest metadata
                script_dir = os.path.dirname(os.path.abspath(__file__))
                persist_dir = os.path.join(script_dir, "chroma_db")
                
                # Always create a fresh database instance to avoid caching issues
                try:
                    # Clear any existing reference first
                    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                    
                    # Force clear any previous instance
                    fresh_db = None
                    gc.collect()
                    
                    # Create a completely new connection
                    fresh_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                    
                    # Get all data from the database
                    db_data = fresh_db.get()
                    
                    if db_data and 'ids' in db_data and db_data['ids']:
                        # Mark database as initialized
                        rag_status['initialized'] = True
                        
                        # Count chunks per document - case insensitive comparison
                        per_doc_counts = {}
                        for meta in db_data.get('metadatas', []):
                            if meta:
                                source_doc = meta.get('source_doc', 'unknown')
                                # Normalize filename for consistent comparison
                                normalized_source = source_doc.strip().lower() if source_doc else 'unknown'
                                per_doc_counts[normalized_source] = per_doc_counts.get(normalized_source, 0) + 1
                                
                                # Also track by original name for display
                                if source_doc and source_doc != 'unknown':
                                    per_doc_counts[source_doc] = per_doc_counts.get(source_doc, 0) + 1
                        
                        # Total chunks in database
                        total_chunks = len(db_data['ids'])
                        rag_status['total_chunk_count'] = total_chunks
                        
                        # Store per-document counts in status
                        rag_status['per_document'] = per_doc_counts
                        
                        # Update file info with chunk counts - case insensitive matching
                        for file_info in files:
                            # First try exact match
                            count = per_doc_counts.get(file_info['name'], 0)
                            
                            # If no exact match, try case-insensitive
                            if count == 0:
                                normalized_name = file_info['name'].strip().lower()
                                count = per_doc_counts.get(normalized_name, 0)
                                
                            file_info['chunk_count'] = count
                        
                        # Find orphaned chunks (chunks for documents that no longer exist)
                        orphaned_sources = {}
                        disk_filenames = set(f['name'] for f in files)
                        for doc_name, count in per_doc_counts.items():
                            if doc_name != 'unknown' and doc_name not in disk_filenames:
                                orphaned_sources[doc_name] = count
                                rag_status['orphaned_chunks'] += count
                        
                        rag_status['orphaned_sources'] = orphaned_sources
                        logging.info(f"[DOCS] Found {len(files)} documents with {total_chunks} total chunks")
                        logging.info(f"[DOCS] Per-document breakdown: {per_doc_counts}")
                    else:
                        logging.warning("[DOCS] No documents found in vector database")
                except Exception as db_err:
                    logging.error(f"[DOCS] Error getting data from vector database: {db_err}", exc_info=True)
                    
                    # Check for orphaned chunks (chunks whose documents no longer exist)
                    actual_filenames = [f['name'] for f in files]
                    orphaned_chunks = 0
                    orphaned_sources = {}
                    
                    for doc_name, count in per_doc_counts.items():
                        if doc_name != 'unknown' and doc_name not in actual_filenames:
                            orphaned_chunks += count
                            orphaned_sources[doc_name] = count
                    
                    # Update RAG status
                    rag_status = {
                        'initialized': True,
                        'total_chunk_count': total_chunks,
                        'per_document': per_doc_counts,
                        'orphaned_chunks': orphaned_chunks,
                        'orphaned_sources': orphaned_sources
                    }
                    
                    logging.info(f"[DOCS] Found {len(files)} documents with {total_chunks} total chunks")
                    logging.info(f"[DOCS] Per-document breakdown: {per_doc_counts}")
                    if orphaned_chunks > 0:
                        logging.warning(f"[DOCS] Found {orphaned_chunks} orphaned chunks from missing documents: {orphaned_sources}")
            except Exception as e:
                logging.error(f"[DOCS] Error getting document chunk counts: {e}", exc_info=True)
        
        return jsonify({
            'documents': files,  # Now includes chunk_count per document
            'rag_status': rag_status
        })
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in docs endpoint: {e}", exc_info=True)
        return jsonify({'documents': [], 'error': str(e)})

# --- Remove a policy document from RAG ---
@ai_agent_bp.route('/docs/<filename>', methods=['DELETE'])
def delete_doc(filename):
    """Delete a policy document and its chunks from the RAG database"""
    global rag_db
    try:
        docs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "policy_docs")
        file_path = os.path.join(docs_dir, filename)
        
        # Check if this is a re-ingestion request
        re_ingest_mode = False
        if request.is_json:
            data = request.get_json()
            re_ingest_mode = data.get('reIngest', False)
        
        logging.info(f"[DELETE] Processing delete request for file: {filename} (reIngest={re_ingest_mode})")
        
        # First, specifically delete chunks for this document (before deleting the file)
        success, message, deleted_count = delete_document_chunks(filename)
        
        if not success:
            logging.error(f"[DELETE] Failed to delete document chunks: {message}")
            return jsonify({
                'success': False, 
                'error': f'Failed to delete document chunks: {message}'
            }), 500
            
        logging.info(f"[DELETE] Successfully deleted {deleted_count} chunks for document: {filename}")
        
        # If this is a re-ingestion, don't actually delete the file
        if re_ingest_mode:
            logging.info(f"[DELETE] Re-ingestion mode: keeping file {filename} for re-processing")
            # Re-ingest by calling ingest_docs.py
            try:
                script_path = os.path.join(os.path.dirname(__file__), 'ingest_docs.py')
                import subprocess
                result = subprocess.run(['python', script_path], 
                                        capture_output=True, 
                                        text=True, 
                                        check=True)
                logging.info(f"[RE-INGEST] Successfully re-ingested documents: {result.stdout}")
                
                # Force reload the RAG database to ensure UI is up-to-date
                try:
                    global rag_db
                    script_dir = os.path.dirname(os.path.abspath(__file__))
                    persist_dir = os.path.join(script_dir, "chroma_db")
                    
                    # Clear existing reference to force reload
                    rag_db = None
                    gc.collect()  # Force garbage collection
                    
                    # Reload with fresh embeddings
                    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                    rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                    logging.info(f"[RE-INGEST] RAG database reloaded after re-ingestion")
                except Exception as reload_err:
                    logging.error(f"[RE-INGEST] Error reloading RAG database: {reload_err}", exc_info=True)
                
                return jsonify({
                    'success': True, 
                    'message': f'Document {filename} was re-ingested successfully'
                })
            except subprocess.CalledProcessError as e:
                logging.error(f"[RE-INGEST] Error re-ingesting documents: {e}")
                return jsonify({
                    'success': False, 
                    'error': f'Failed to re-ingest documents: {str(e)}'
                }), 500
        
        # Otherwise, delete the file as requested
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"[DELETE] Deleted file: {file_path}")
            else:
                logging.warning(f"[DELETE] File not found (already deleted?): {file_path}")
            
            # Force reload the RAG database to ensure UI is up-to-date
            try:
                global rag_db
                script_dir = os.path.dirname(os.path.abspath(__file__))
                persist_dir = os.path.join(script_dir, "chroma_db")
                
                # Clear existing reference to force reload
                rag_db = None
                gc.collect()  # Force garbage collection
                
                # Reload with fresh embeddings
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                logging.info(f"[DELETE] RAG database reloaded after deletion")
            except Exception as reload_err:
                logging.error(f"[DELETE] Error reloading RAG database: {reload_err}", exc_info=True)
                
            return jsonify({
                'success': True, 
                'message': f'Document {filename} and {deleted_count} chunks were deleted successfully'
            })
        except Exception as e:
            logging.error(f"[DELETE] Error deleting file {file_path}: {e}", exc_info=True)
            return jsonify({
                'success': False, 
                'error': f'Document chunks were deleted but file removal failed: {str(e)}'
            }), 500
            
    except Exception as e:
        logging.error(f"[DELETE] Unhandled error deleting document {filename}: {e}", exc_info=True)
        return jsonify({
            'success': False, 
            'error': f'Error deleting document: {str(e)}'
        }), 500
@app.route('/ai-agent/docs/<filename>', methods=['DELETE'])
def delete_doc(filename):
    """Delete a policy document and its chunks from the RAG database"""
    try:
        docs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "policy_docs")
        file_path = os.path.join(docs_dir, filename)
        
        # Check if this is a re-ingestion request
        re_ingest_mode = False
        if request.is_json:
            data = request.get_json()
            re_ingest_mode = data.get('reIngest', False)
        
        logging.info(f"[DELETE] Processing delete request for file: {filename} (reIngest={re_ingest_mode})")
        
        # Implement document chunk deletion directly here
        success = False
        message = ""
        deleted_count = 0
        
        # Delete chunks code inline
        try:
            if rag_db is None:
                logging.warning(f"[DELETE-CHUNKS] No RAG database loaded, nothing to delete for {filename}")
                message = "No RAG database loaded"
            else:
                # Get all document IDs and metadatas
                logging.info(f"[DELETE-CHUNKS] Preparing to delete chunks for document: {filename}")
                db_data = rag_db.get()
                
                if not db_data or 'ids' not in db_data or not db_data['ids']:
                    logging.warning(f"[DELETE-CHUNKS] RAG database is empty, nothing to delete")
                    message = "Database is empty, nothing to delete"
                    success = True
                else:
                    # Find IDs of chunks that belong to the document being deleted
                    chunk_ids_to_delete = []
                    for i, meta in enumerate(db_data.get('metadatas', [])):
                        source_doc = meta.get('source_doc', 'unknown')
                        
                        # If this chunk belongs to the document we're deleting
                        if source_doc == filename:
                            if i < len(db_data.get('ids', [])):
                                chunk_ids_to_delete.append(db_data['ids'][i])
                                
                    # If no chunks found for this document
                    if not chunk_ids_to_delete:
                        logging.warning(f"[DELETE-CHUNKS] No chunks found for document: {filename}")
                        message = f"No chunks found for document {filename}"
                        success = True
                    else:
                        # Delete the chunks from the vector database - handle readonly database
                        logging.info(f"[DELETE-CHUNKS] Attempting to delete {len(chunk_ids_to_delete)} chunks for document: {filename}")
                        try:
                            # First try direct deletion which may fail with readonly database
                            rag_db.delete(ids=chunk_ids_to_delete)
                            logging.info(f"[DELETE-CHUNKS] Successfully deleted chunks using direct method")
                            success = True
                        except Exception as delete_err:
                            if "readonly database" in str(delete_err).lower():
                                # Handle readonly database by rebuilding
                                logging.warning(f"[DELETE-CHUNKS] Readonly database detected, using alternative deletion method")
                                
                                # Delete the file first and then manually trigger reingestion
                                if os.path.exists(file_path) and not re_ingest_mode:
                                    try:
                                        os.remove(file_path)
                                        logging.info(f"[DELETE] Successfully deleted file: {filename}")
                                        
                                        # Mark as successful even though we didn't delete the chunks directly
                                        # They'll be removed in the next ingestion
                                        message = f"File deleted but database is readonly. Chunks will be removed on next ingestion."
                                        deleted_count = len(chunk_ids_to_delete)
                                        success = True
                                    except Exception as file_err:
                                        logging.error(f"[DELETE] Error deleting file {filename}: {file_err}")
                                        return jsonify({
                                            'success': False, 
                                            'error': f'Failed to delete file: {str(file_err)}'
                                        }), 500
                                else:
                                    # For re-ingestion or if file doesn't exist
                                    message = f"Database is readonly. Chunks will be updated on next ingestion."
                                    deleted_count = 0
                                    success = True
                            else:
                                # Re-raise for other errors
                                raise
        except Exception as e:
            logging.error(f"[DELETE-CHUNKS] Error deleting chunks for {filename}: {e}", exc_info=True)
            message = f"Error deleting chunks: {str(e)}"
            success = False
        
        if not success:
            logging.error(f"[DELETE] Failed to delete document chunks: {message}")
            return jsonify({
                'success': False, 
                'error': f'Failed to delete document chunks: {message}'
            }), 500
            
        logging.info(f"[DELETE] Successfully deleted {deleted_count} chunks for document: {filename}")
        
        # If this is a re-ingestion, don't actually delete the file
        if re_ingest_mode:
            logging.info(f"[DELETE] Re-ingestion mode: keeping file {filename} for re-processing")
            return jsonify({
                'success': True, 
                'message': f'Successfully deleted {deleted_count} chunks and kept file for re-ingestion',
                'deleted_chunks': deleted_count,
                'reingested': True
            })
            
        # Then delete the actual file
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                logging.info(f"[DELETE] Successfully deleted file: {filename}")
            except Exception as file_err:
                logging.error(f"[DELETE] Error deleting file {filename}: {file_err}")
                return jsonify({
                    'success': False, 
                    'error': f'Successfully deleted {deleted_count} chunks but failed to delete file: {str(file_err)}'
                }), 500
        else:
            logging.warning(f"[DELETE] File not found on disk: {filename}, but deleted from RAG")
            
        return jsonify({
            'success': True, 
            'message': f'Successfully deleted document {filename} and {deleted_count} chunks',
            'deleted_chunks': deleted_count
        })
    except Exception as e:
        logging.error(f"[DELETE] Unhandled error deleting document {filename}: {e}", exc_info=True)
        return jsonify({
            'success': False, 
            'error': f'Error deleting document: {str(e)}'
        }), 500
        
        if not success:
            logging.error(f"[DELETE] Failed to delete document chunks: {message}")
            return jsonify({
                'success': False, 
                'error': f'Failed to delete document chunks: {message}'
            }), 500
            
        logging.info(f"[DELETE] Successfully deleted {deleted_count} chunks for document: {filename}")
        
        # If this is a re-ingestion, don't actually delete the file
        if re_ingest_mode:
            logging.info(f"[DELETE] Re-ingestion mode: keeping file {filename} for re-processing")
            # Re-ingest by calling ingest_docs.py
            try:
                script_path = os.path.join(os.path.dirname(__file__), 'ingest_docs.py')
                import subprocess
                result = subprocess.run(['python', script_path], 
                                        capture_output=True, 
                                        text=True, 
                                        check=True)
                logging.info(f"[RE-INGEST] Successfully re-ingested documents: {result.stdout}")
                return jsonify({
                    'success': True, 
                    'message': f'Document {filename} was re-ingested successfully'
                })
            except subprocess.CalledProcessError as e:
                logging.error(f"[RE-INGEST] Error re-ingesting documents: {e}")
                return jsonify({
                    'success': False, 
                    'error': f'Failed to re-ingest documents: {str(e)}'
                }), 500
        
        # Otherwise, delete the file as requested
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"[DELETE] Deleted file: {file_path}")
            else:
                logging.warning(f"[DELETE] File not found (already deleted?): {file_path}")
                
            return jsonify({
                'success': True, 
                'message': f'Document {filename} and {deleted_count} chunks were deleted successfully'
            })
        except Exception as e:
            logging.error(f"[DELETE] Error deleting file {file_path}: {e}", exc_info=True)
            return jsonify({
                'success': False, 
                'error': f'Document chunks were deleted but file removal failed: {str(e)}'
            }), 500
            
    except Exception as e:
        logging.error(f"[DELETE] Unhandled error deleting document {filename}: {e}", exc_info=True)
        return jsonify({
            'success': False, 
            'error': f'Error deleting document: {str(e)}'
        }), 500

# Health check endpoint
@ai_agent_bp.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'ok'}), 200

@ai_agent_bp.route('/upload', methods=['POST'])
def upload():
    global rag_db
    if 'file' not in request.files:
        logging.error("[UPLOAD] No file part in the request")
        return jsonify({'success': False, 'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        logging.error("[UPLOAD] No selected file")
        return jsonify({'success': False, 'error': 'No selected file'}), 400
        
    filename = secure_filename(file.filename)
    save_path = os.path.join(app.config['POLICY_UPLOAD_FOLDER'], filename)
    
    try:
        # Ensure policy docs directory exists
        os.makedirs(app.config['POLICY_UPLOAD_FOLDER'], exist_ok=True)
        
        # Save the file
        file.save(save_path)
        logging.info(f"[UPLOAD] Saved file to {save_path}")
        
        # Verify file was saved successfully
        if not os.path.exists(save_path):
            raise FileNotFoundError(f"File was not saved at {save_path}")
            
        file_size = os.path.getsize(save_path)
        logging.info(f"[UPLOAD] Verified file exists at {save_path} with size {file_size} bytes")
    except Exception as save_error:
        logging.error(f"[UPLOAD] Error saving file: {save_error}", exc_info=True)
        return jsonify({'success': False, 'error': f'Error saving file: {str(save_error)}'}), 500
    
    # Automatically ingest after upload
    try:
        logging.info("[UPLOAD] Running document ingestion...")
        import subprocess
        script_path = os.path.join(os.path.dirname(__file__), 'ingest_docs.py')
        
        # Run ingestion script with additional logging
        logging.info(f"[UPLOAD] Running ingestion script at {script_path}")
        
        # Run with full environment
        env = os.environ.copy()
        result = subprocess.run([
            'python', script_path
        ], check=True, capture_output=True, text=True, env=env)
        
        logging.info(f"[UPLOAD] Ingestion completed with output: {result.stdout}")
        if result.stderr:
            logging.warning(f"[UPLOAD] Ingestion warnings: {result.stderr}")
        
        # Wait a moment to ensure database updates are complete
        time.sleep(1.5)  # Increased wait time for filesystem sync
        
        # Verify the document was actually ingested
        try:
            # Check if the file is in the RAG database
            docs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "policy_docs")
            file_path = os.path.join(docs_dir, filename)
            
            # Force reload the RAG database with garbage collection
            global rag_db
            persist_dir = os.path.join(os.path.dirname(__file__), 'chroma_db')
            
            # Check if the database exists first
            if not os.path.exists(persist_dir):
                logging.error(f"[UPLOAD] Vector database directory not found at {persist_dir}")
                return jsonify({
                    'success': False, 
                    'error': f'Document saved but RAG database directory not found'
                }), 500
                
            # Re-initialize embeddings to ensure they match what was used during ingestion
            local_embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
            
            # Fix permissions on database directory before loading
            try:
                if os.path.exists(persist_dir):
                    os.chmod(persist_dir, 0o777)
                    # Fix permissions on all files and directories
                    for root, dirs, files in os.walk(persist_dir):
                        for d in dirs:
                            try:
                                os.chmod(os.path.join(root, d), 0o777)
                            except Exception as e:
                                logging.warning(f"[UPLOAD] Could not set permissions on directory {d}: {e}")
                        for f in files:
                            try:
                                os.chmod(os.path.join(root, f), 0o666)
                            except Exception as e:
                                logging.warning(f"[UPLOAD] Could not set permissions on file {f}: {e}")
            except Exception as perm_err:
                logging.warning(f"[UPLOAD] Permission fix warning (non-fatal): {perm_err}")
            
            # Important: Create a new Chroma instance to avoid caching issues
            rag_db = None  # Clear the existing reference to force reload
            gc.collect()  # Force garbage collection
            rag_db = Chroma(persist_directory=persist_dir, embedding_function=local_embeddings)
            
            # Verify the database exists and try to query it
            try:
                # Execute a test query to check database functionality
                db_data = rag_db.get()
                
                # Check if database has any data
                if not db_data:
                    logging.error("[UPLOAD] RAG database exists but could not retrieve any data")
                    return jsonify({
                        'success': False,
                        'error': 'Document saved but RAG database contains no data after ingestion'
                    }), 500
                
                # Check for document chunks - case insensitive matching
                found_chunks = False
                doc_chunks = 0
                
                if 'metadatas' in db_data and db_data['metadatas']:
                    # We have some metadata, check if our document is there (case insensitive)
                    normalized_filename = filename.strip().lower()
                    for meta in db_data['metadatas']:
                        if meta and 'source_doc' in meta:
                            doc_name = meta['source_doc']
                            # Try both exact match and case-insensitive match
                            if doc_name == filename or doc_name.strip().lower() == normalized_filename:
                                found_chunks = True
                                doc_chunks += 1
                
                if found_chunks:
                    logging.info(f"[UPLOAD] Document {filename} successfully found in RAG database with {doc_chunks} chunks")
                    return jsonify({
                        'success': True, 
                        'message': f'Document {filename} uploaded and processed successfully with {doc_chunks} chunks',
                        'fileSize': file_size,
                        'filename': filename,
                        'chunks': doc_chunks
                    })
                else:
                    # Try to check for any chunks to verify database is working
                    if 'metadatas' in db_data and db_data['metadatas'] and len(db_data['metadatas']) > 0:
                        # Database has content but not our document
                        logging.warning(f"[UPLOAD] Database has {len(db_data['metadatas'])} chunks but none for {filename}")
                        
                        # List all sources in database for debugging
                        sources = set()
                        for meta in db_data['metadatas']:
                            if meta and 'source_doc' in meta:
                                sources.add(meta['source_doc'])
                        logging.info(f"[UPLOAD] Sources in database: {sources}")
                        
                        # Try running ingestion one more time
                        try:
                            logging.info("[UPLOAD] Attempting re-ingestion due to missing document...")
                            retry_result = subprocess.run([
                                'python', script_path
                            ], check=True, capture_output=True, text=True, env=env)
                            
                            logging.info(f"[UPLOAD] Re-ingestion completed with output: {retry_result.stdout}")
                            
                            # Wait for filesystem sync
                            time.sleep(2)
                            
                            # Clear and reload the database again
                            rag_db = None
                            gc.collect()
                            rag_db = Chroma(persist_directory=persist_dir, embedding_function=local_embeddings)
                            
                            # Check again for the document
                            retry_data = rag_db.get()
                            retry_chunks = 0
                            if retry_data and 'metadatas' in retry_data:
                                normalized_filename = filename.strip().lower()
                                for meta in retry_data['metadatas']:
                                    if meta and 'source_doc' in meta:
                                        doc_name = meta['source_doc']
                                        if doc_name == filename or doc_name.strip().lower() == normalized_filename:
                                            retry_chunks += 1
                            
                            if retry_chunks > 0:
                                logging.info(f"[UPLOAD] Document found after re-ingestion with {retry_chunks} chunks")
                                return jsonify({
                                    'success': True, 
                                    'message': f'Document {filename} uploaded and processed successfully with {retry_chunks} chunks after retry',
                                    'fileSize': file_size,
                                    'filename': filename,
                                    'chunks': retry_chunks
                                })
                        except Exception as retry_err:
                            logging.error(f"[UPLOAD] Re-ingestion attempt failed: {retry_err}", exc_info=True)
                        
                        # Return success anyway, since ingestion process reported success
                        # The document might take a moment to show up due to async processing
                        return jsonify({
                            'success': True, 
                            'message': f'Document {filename} uploaded and processed. Database contains {len(db_data["metadatas"])} chunks from other documents.',
                            'fileSize': file_size,
                            'filename': filename,
                            'chunks': 0,
                            'warning': 'Document may not be immediately available in search results'
                        })
                    else:
                        # No documents at all in database - this could indicate an ingestion failure
                        logging.error(f"[UPLOAD] RAG database contains no document chunks after ingestion")
                        return jsonify({
                            'success': False,
                            'error': 'Document saved but no document chunks found in RAG database after ingestion'
                        }), 500
            except Exception as query_err:
                logging.error(f"[UPLOAD] Error querying RAG database: {query_err}", exc_info=True)
                
                # Since ingestion reported success but verification failed, return a warning
                return jsonify({
                    'success': True, 
                    'message': f'Document {filename} uploaded, but verification could not confirm ingestion',
                    'fileSize': file_size,
                    'filename': filename,
                    'warning': 'Document may not be immediately available in search results'
                })
        except Exception as verify_err:
            logging.error(f"[UPLOAD] Failed to verify document ingestion: {verify_err}", exc_info=True)
            return jsonify({
                'success': False, 
                'error': f'Document saved but failed to verify ingestion: {str(verify_err)}'
            }), 500
    except subprocess.CalledProcessError as proc_err:
        logging.error(f"[UPLOAD] Ingestion process error: {proc_err}", exc_info=True)
        if proc_err.stderr:
            logging.error(f"[UPLOAD] Ingestion stderr: {proc_err.stderr}")
        return jsonify({
            'success': False, 
            'error': f'Upload succeeded but ingestion failed. Process error: {proc_err.stderr or str(proc_err)}'
        })
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[UPLOAD] Error during ingestion: {e}", exc_info=True)
        return jsonify({
            'success': False, 
            'error': f'Upload succeeded but ingestion failed: {str(e)}'
        })

@ai_agent_bp.route('/rag', methods=['POST'])
def rag():
    # Add test response to confirm the endpoint is reachable
    try:
        logging.info("[RAG_DEBUG] Starting RAG request at " + str(datetime.now()))
        if request.json:
            logging.info(f"[RAG_DEBUG] Request JSON: {request.json}")
        else:
            logging.info("[RAG_DEBUG] No request.json found")
        # Test early return to see if the function is being called properly
        if request.json and request.json.get("test_mode") == "true":
            return jsonify({"status": "rag_endpoint_reachable", "message": "RAG endpoint is functioning correctly"})
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[RAG_DEBUG] Error in debug section: {e}", exc_info=True)
        return jsonify({"error": f"Debug error: {str(e)}"}), 500

    logging.info("[RAG] Starting RAG request")
    logging.info(f"[RAG] Request JSON: {request.json}")
    user_input = request.json.get('input')
    
    # Check if 'input' parameter is provided, if not, check for 'query' parameter
    if not user_input and request.json:
        user_input = request.json.get('query')
        logging.info(f"[RAG] No 'input' parameter found, using 'query' parameter: {user_input}")
    
    if not user_input:
        return jsonify({'response': 'No question provided. Please include "input" or "query" parameter.'}), 400
        
    # Check if RAG database is loaded
    if rag_db is None:
        return jsonify({
            'response': 'The policy knowledge base is not loaded. Please upload policy documents first.',
            'error': 'rag_not_loaded'
        })
        
    try:
        # Check if the database has documents
        db_data = rag_db.get()
        if not db_data or 'documents' not in db_data or not db_data['documents']:
            return jsonify({
                'response': 'The policy knowledge base contains no documents. Please upload policy documents first.',
                'error': 'no_documents'
            })
            
        # Get similar documents
        docs = rag_db.similarity_search(user_input, k=3)
        if not docs:
            return jsonify({
                'response': 'I couldn\'t find any relevant policy information to answer your question. Try asking about a different topic or upload more relevant policy documents.',
                'error': 'no_relevant_docs'
            })
            
        # Enhanced diagnostic information about retrieved chunks
        chunk_info = []
        for i, doc in enumerate(docs):
            source_doc = doc.metadata.get('source_doc', 'unknown')
            chunk_size = doc.metadata.get('chunk_size', len(doc.page_content))
            
            # Log detailed chunk information
            chunk_info.append({
                'index': i,
                'source_doc': source_doc,
                'chunk_size': chunk_size,
                'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content,
                'metadata': doc.metadata
            })
            
        # Log detailed diagnostic information about the retrieved chunks
        logging.info(f"[RAG] Retrieved {len(docs)} chunks from database")
        logging.info(f"[RAG] Chunk details: {json.dumps(chunk_info, indent=2)}")
        
        # Create context from documents
        context = "\n\n".join([d.page_content for d in docs])
        
        # Create prompt
        prompt = f"""Use the following DWP policy context to answer the question. 
If the context doesn't contain relevant information to answer the question, 
say so clearly and suggest what other information might be needed.

POLICY CONTEXT:
{context}

QUESTION: {user_input}

If possible, cite the specific policy or document section that contains your answer."""
        
        # Log the prompt
        logging.info(f"[RAG] Using prompt with {len(docs)} documents, prompt length: {len(prompt)}")
        
        # Call LLM
        response = llm.invoke(prompt)
        
        # Extract response content
        if hasattr(response, 'content'):
            response_content = response.content
        else:
            response_content = str(response)
            
        logging.info(f"[RAG] Generated response length: {len(response_content)}")
        
        # Return response
        return jsonify({"response": response_content})
        
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[RAG] Error: {e}", exc_info=True)
        return jsonify({
            'response': f"I encountered an error while searching the policy knowledge base. Please try again later.",
            'error': str(e)
        }), 500

#Funny prompt
funny_prompt =  os.environ.get('funny_prompt')

# In-memory user session storage (simple dictionary)
user_sessions = {}

# 1. Define the conversation state
class ConversationState(TypedDict):
    input: str
    history: str = ""
    response: str = None
    need_search: bool = False
    search_results: str = ""
    RAG: bool 


# 2. Initialize the LLM and search tool
try:
    llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_key)
    logging.info("[INIT] Successfully initialized ChatOpenAI")
except Exception as e:
    logging.error(f"[INIT] Error initializing ChatOpenAI: {e}", exc_info=True)
    llm = None  # We'll handle this case in the endpoints

if tavily_key:
    try:
        search_tool = TavilySearchResults(api_key=tavily_key)
        logging.info("[INIT] Successfully initialized TavilySearchResults")
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[INIT] Error initializing TavilySearchResults: {e}", exc_info=True)
        search_tool = None
else:
    logging.warning("[INIT] TAVILY_API_KEY not set in environment. Web search will not work.")
    search_tool = None
    
need_to_search = False

# 3. LangGraph nodes
def decide_search(state: ConversationState) -> ConversationState:
    global need_to_search
    question = f"""
    Your task is to determine whether a question requires a web search to answer accurately and completely.

    If the question can be answered without up-to-date or external data (e.g. general knowledge), respond: No.  
    If the question requires current, location-specific, or real-time data, respond: Yes. 
    
    Question: {state['input']} 
    """
    decision = llm.predict(question)
    state['need_search'] = "yes" in decision.lower()
    if state['need_search']:
        need_to_search = True
    else:
        need_to_search = False    
    return state

def perform_search(state: ConversationState) -> ConversationState:
    logging.info(f"[perform_search] need_search: {state['need_search']}")
    if state['need_search']:
        try:
            logging.info(f"[perform_search] Attempting web search for: {state['input']}")
            results = search_tool.run(state['input'])
            logging.info(f"[perform_search] Web search results: {results}")
            if not results or (isinstance(results, str) and not results.strip()):
                logging.error("[perform_search] No results returned from Tavily API.")
                state['search_results'] = '[Web search error: No results returned from Tavily API]'
            else:
                state['search_results'] = results
        except Exception as e:
            logging.error(f"[perform_search] Web search failed: {e}", exc_info=True)
            state['search_results'] = f"[Web search error: {e}]"
    else:
        logging.info("[perform_search] Web search not needed.")
    return state

def generate_response(state: ConversationState) -> ConversationState:
    full_prompt = f"{state['history']}\nYou: {state['input']}"
    if state['search_results']:
        full_prompt += f"\n\nSearch results:\n{state['search_results']}"
    
    # Add funny prompt if configured
    if funny_prompt:
        full_prompt = funny_prompt + full_prompt
    
    # If web search failed, return a clear error to the user
    if '[Web search error:' in (state['search_results'] or ''):
        state['response'] = state['search_results']
        state['history'] += f"\nYou: {state['input']}\nAssistant: {state['search_results']}"
        return state
    
    try:
        logging.info(f"[GEN_RESP] Calling llm.invoke() with prompt: {full_prompt}")
        response = llm.invoke(full_prompt)
        logging.info(f"[GEN_RESP] llm.invoke() returned response object type: {type(response)}")
        
        # Extract content from response object
        if hasattr(response, 'content'):
            response_content = response.content
        else:
            response_content = str(response)
            
        logging.info(f"[GEN_RESP] Extracted response content: {response_content}")
    except Exception as llm_exc:
        logging.error(f"[GEN_RESP] llm.invoke() error: {llm_exc}", exc_info=True)
        response_content = f"I encountered an error while generating a response. Please try again or rephrase your question."
    
    if not response_content:
        logging.error("[GEN_RESP] llm.invoke() returned empty or None response.")
        response_content = "I couldn't generate a response for your question. Please try again or ask something different."
        
    clean_response = response_content.replace("Assistant:", "").strip() if isinstance(response_content, str) else str(response_content)
    state['response'] = clean_response
    state['history'] += f"\nYou: {state['input']}\nAssistant: {clean_response}"
    return state

# 4. Build the LangGraph
builder = StateGraph(ConversationState)
builder.add_node("decide_search", decide_search)
builder.add_node("perform_search", perform_search)
builder.add_node("generate_response", generate_response)
builder.set_entry_point("decide_search")
builder.add_edge("decide_search", "perform_search")
builder.add_edge("perform_search", "generate_response")
builder.add_edge("generate_response", END)
graph = builder.compile()

# 5. Flask route for the UI with background image





@ai_agent_bp.route('/')
def home():
    return render_template("index.html")

@ai_agent_bp.route('/chat', methods=['POST'])
def chat():
    try:
        user_input = request.json.get('input', None)
        session_id = request.remote_addr
        history = user_sessions.get(session_id, "")
        logging.info(f"[CHAT] Received chat request: {user_input}")
        if not user_input:
            logging.error("[CHAT] No input provided in request body.")
            return jsonify({"response": "[Error: No input provided]"}), 400

        # Log RAG DB and LLM status
        logging.info(f"[CHAT] rag_db is {'set' if rag_db is not None else 'NOT set'}.")
        logging.info(f"[CHAT] tavily_key is {'set' if tavily_key else 'NOT set'}.")
        logging.info(f"[CHAT] openai_key is {'set' if openai_key else 'NOT set'}.")

        if not openai_key:
            logging.error("[CHAT] OpenAI API key is not set.")
            return jsonify({"response": "Configuration error: OpenAI API key is not set. Please check server configuration."}), 500

        # First, check if RAG is available
        rag_available = False
        rag_document_count = 0
        
        if rag_db is not None:
            try:
                db_data = rag_db.get()
                if db_data and 'documents' in db_data and len(db_data['documents']) > 0:
                    rag_available = True
                    rag_document_count = len(db_data['documents'])
                    logging.info(f"[CHAT] RAG is available with {rag_document_count} chunks")
                else:
                    logging.info("[CHAT] RAG database is empty")
            except Exception as e:
                logging.error(f"[CHAT] Error checking RAG database: {e}", exc_info=True)
        else:
            logging.info("[CHAT] RAG database is not initialized")
        
        # Determine if we should use RAG based on the query and availability
        use_rag = False
        if rag_available:
            # Simple heuristic: if "policy" or related terms in question, use RAG
            policy_keywords = ["policy", "dwp", "regulation", "benefit", "funeral", "payment", 
                              "document", "documentation", "guidelines", "rules", "assistance"]
            
            if any(word.lower() in user_input.lower() for word in policy_keywords):
                use_rag = True
                logging.info("[CHAT] Using RAG based on query keywords")
            else:
                logging.info("[CHAT] Query doesn't match RAG keywords, using web search")
        
        # Try RAG first if available and applicable
        if use_rag:
            try:
                logging.info("[CHAT] Using RAG agent")
                # Get relevant chunks from RAG
                docs = rag_db.similarity_search(user_input, k=3)
                
                if not docs:
                    logging.info("[CHAT] No relevant documents found in RAG database, falling back to web search")
                    use_rag = False
                else:
                    # Enhanced diagnostic information about retrieved chunks
                    chunk_info = []
                    for i, doc in enumerate(docs):
                        source_doc = doc.metadata.get('source_doc', 'unknown')
                        chunk_size = doc.metadata.get('chunk_size', len(doc.page_content))
                        
                        # Log detailed chunk information
                        chunk_info.append({
                            'index': i,
                            'source_doc': source_doc,
                            'chunk_size': chunk_size,
                            'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content,
                            'metadata': doc.metadata
                        })
                        
                    # Log detailed diagnostic information about the retrieved chunks
                    logging.info(f"[CHAT] Retrieved {len(docs)} chunks from RAG database")
                    logging.info(f"[CHAT] RAG chunk details: {json.dumps(chunk_info, indent=2)}")
                    
                    context = "\n\n".join([d.page_content for d in docs])
                    
                    # Create an informative prompt
                    prompt = f"""Use the following DWP policy context to answer the question. 
If the context doesn't contain relevant information to answer the question, 
say so clearly and suggest what other information might be needed.

POLICY CONTEXT:
{context}

QUESTION: {user_input}

If possible, cite the specific policy or document section that contains your answer."""
                    
                    logging.info(f"[CHAT] RAG prompt created with {len(docs)} chunks")
                    
                    # Get response from LLM
                    try:
                        response = llm.invoke(prompt)
                        if hasattr(response, 'content'):
                            response_content = response.content
                        else:
                            response_content = str(response)
                        
                        logging.info(f"[CHAT] RAG LLM returned response of length {len(response_content)}")
                        
                        # Update session history
                        user_sessions[session_id] = history + f"\nYou: {user_input}\nAssistant: {response_content}"
                        
                        return jsonify({"response": response_content, "source": "rag"})
                    except Exception as llm_err:
                        logging.error(f"[CHAT] RAG LLM error: {llm_err}", exc_info=True)
                        # Fall back to web search if RAG fails
                        logging.info("[CHAT] Falling back to web search due to RAG failure")
                        use_rag = False
            except Exception as rag_err:
                logging.error(f"[CHAT] RAG processing error: {rag_err}", exc_info=True)
                # Fall back to web search if RAG fails
                logging.info("[CHAT] Falling back to web search due to RAG error")
                use_rag = False
        
        # Use web search if RAG is not available or applicable
        if not use_rag:
            try:
                logging.info("[CHAT] Using web search agent")
                
                # Initialize state for LangGraph
                state = ConversationState(input=user_input, history=history, search_results="", need_search=False, RAG=False)
                logging.info(f"[CHAT] Web agent initial state created")
                
                try:
                    # Invoke the graph
                    result = graph.invoke(state)
                    
                    # Update session history
                    user_sessions[session_id] = result['history']
                    
                    # Get whether search was used
                    need_search = result.get('need_search', False)
                    
                    # Check for valid response
                    if not result or not result.get('response'):
                        logging.error("[CHAT] Web agent graph.invoke() returned empty response")
                        
                        # Fallback to direct LLM call if graph fails
                        try:
                            direct_response = llm.invoke(f"Answer this question concisely: {user_input}")
                            
                            if hasattr(direct_response, 'content'):
                                direct_content = direct_response.content
                            else:
                                direct_content = str(direct_response)
                            
                            logging.info(f"[CHAT] Fallback direct LLM response: {direct_content}")
                            return jsonify({"response": direct_content, "source": "direct_llm"})
                        except Exception as fallback_err:
                            logging.error(f"[CHAT] Fallback LLM error: {fallback_err}", exc_info=True)
                            return jsonify({
                                "response": "I'm having trouble generating a response right now. Please try again later.",
                                "source": "error"
                            }), 500
                    
                    # Format the response with a prefix if search was used
                    response_prefix = "Searching for information... " if need_search else ""
                    response_text = response_prefix + result['response']
                    
                    logging.info(f"[CHAT] Web agent response: {response_text[:100]}...")
                    return jsonify({"response": response_text, "source": "web"})
                    
                except Exception as graph_err:
                    logging.error(f"[CHAT] Error in graph.invoke(): {graph_err}", exc_info=True)
                    
                    # Fallback to direct LLM call if graph fails
                    try:
                        direct_response = llm.invoke(f"Answer this question concisely: {user_input}")
                        
                        if hasattr(direct_response, 'content'):
                            direct_content = direct_response.content
                        else:
                            direct_content = str(direct_response)
                        
                        logging.info(f"[CHAT] Fallback direct LLM response: {direct_content}")
                        return jsonify({"response": direct_content, "source": "direct_llm"})
                    except Exception as fallback_err:
                        logging.error(f"[CHAT] Fallback LLM error: {fallback_err}", exc_info=True)
                        return jsonify({
                            "response": "I'm having trouble generating a response right now. Please try again later.",
                            "source": "error"
                        }), 500
            
            except Exception as web_err:
                logging.error(f"[CHAT] Web agent error: {web_err}", exc_info=True)
                return jsonify({
                    "response": "I'm having trouble accessing my knowledge sources. Please try again later.",
                    "source": "error"
                }), 500
                
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[CHAT] General chat endpoint error: {e}", exc_info=True)
        return jsonify({
            "response": "Something went wrong while processing your request. Please try again later.",
            "source": "error"
        }), 500

@ai_agent_bp.route('/check-form', methods=['POST'])
def check_form():
    try:
        content = request.json.get('content', '')
        logging.info(f"[CHECK-FORM] Received form data length: {len(content)}")
        
        # Verify llm is properly initialized
        if llm is None:
            logging.error("[CHECK-FORM] LLM not initialized properly")
            return jsonify({"response": "Error: AI model not available. Check OpenAI API key configuration."}), 500
        
        # Prompt for policy verification
        policy_prompt = (
            "You are a DWP policy expert. Review the following form questions and answers. "
            "Identify any answers that do not comply with DWP policy or may need amending. "
            "Suggest improvements or flag any issues.\n\n" + content
        )
        # Use RAG if available
        if rag_db is not None:
            docs = rag_db.similarity_search(content, k=3)
            context = "\n\n".join([d.page_content for d in docs])
            policy_prompt = (
                f"Use the following DWP policy context to check the form:\n{context}\n\n" + policy_prompt
            )
        response = llm.invoke(policy_prompt)
        # Ensure the response is JSON serializable (convert to string if needed)
        # If response is an object (e.g., AIMessage), convert to string
        try:
            response_str = str(response.content)
        except AttributeError:
            response_str = str(response)
        logging.info(f"[CHECK-FORM] Response generated successfully. Length: {len(response_str)}")
        return jsonify({"response": response_str})
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f"[CHECK-FORM] Error: {e}", exc_info=True)
        return jsonify({"response": f"Error processing form: {str(e)}"}), 500

# --- OCR API Endpoint ---
@ai_agent_bp.route('/ocr/process', methods=['POST'])
def process_ocr_document():
    """Process a document with OCR and return extracted text"""
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
        
    # Initialize document processor
    document_processor_instance = document_processor.DocumentProcessor(upload_folder=app.config['UPLOAD_FOLDER'])
    
    # Save and process the file
    file_path = document_processor_instance.save_uploaded_file(file)
    if not file_path:
        return jsonify({"error": "Failed to save file"}), 500
        
    # Process the document
    result = document_processor_instance.process_file(file_path)
    
    # Return the result
    return jsonify(result)

# --- Batch OCR Processing ---
@ai_agent_bp.route('/ocr/batch', methods=['POST'])
def batch_process_ocr_documents():
    """Process multiple documents with OCR"""
    if 'files' not in request.files:
        return jsonify({"error": "No files part"}), 400
        
    files = request.files.getlist('files')
    if not files or files[0].filename == '':
        return jsonify({"error": "No selected files"}), 400
        
    # Initialize document processor
    document_processor_instance = document_processor.DocumentProcessor(upload_folder=app.config['UPLOAD_FOLDER'])
    
    file_paths = []
    for file in files:
        file_path = document_processor_instance.save_uploaded_file(file)
        if file_path:
            file_paths.append(file_path)
            
    # Process all documents
    results = document_processor_instance.batch_process_files(file_paths)
    
    # Return the results
    return jsonify(results)
    
# --- AI Document Analysis ---
@ai_agent_bp.route('/ocr/analyze', methods=['POST'])
def analyze_ocr_document():
    """Process a document with OCR and analyze its content with AI"""
    
    # Support direct JSON input for text analysis
    if request.is_json:
        json_data = request.get_json()
        if not json_data or 'text' not in json_data:
            return jsonify({"error": "Missing 'text' field in JSON data"}), 400
            
        try:
            # Initialize AI document processor
            ai_processor = ai_document_processor.AIDocumentProcessor()
            
            # Extract info from text directly
            result = ai_processor.analyze_text_directly(
                json_data['text'],
                json_data.get('document_type', 'generic'),
                json_data.get('fields_to_extract', [])
            )
            
            return jsonify(result)
        except Exception as e:
            logging.error(f"[OCR] Error analyzing JSON text: {e}", exc_info=True)
            return jsonify({"error": f"Error analyzing text: {str(e)}"}), 500
    
    # File upload path
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
        
    # Get custom queries if provided
    queries = None
    if request.form and 'queries' in request.form:
        try:
            queries = json.loads(request.form['queries'])
        except Exception as e:
            logging.error(f"[OCR] Error parsing queries: {e}", exc_info=True)
            
    # Initialize document processor
    document_processor_instance = document_processor.DocumentProcessor(upload_folder=app.config['UPLOAD_FOLDER'])
    
    # Save the file
    file_path = document_processor_instance.save_uploaded_file(file)
    if not file_path:
        return jsonify({"error": "Failed to save file"}), 500
        
    try:
        # Initialize AI document processor
        ai_processor = ai_document_processor.AIDocumentProcessor()
        
        # Process and analyze the document
        result = ai_processor.process_and_analyze_document(file_path, queries)
        
        # Return the analysis result
        return jsonify(result)
    except Exception as e:
        logging.error(f"[OCR] Error analyzing document: {e}", exc_info=True)
        return jsonify({"error": f"Error analyzing document: {str(e)}"}), 500

@ai_agent_bp.route('/debug-rag', methods=['GET'])
def debug_rag():
    try:
        # Get database status
        if rag_db is None:
            return jsonify({'status': 'not_loaded', 'error': 'RAG database is not loaded'})
            
        # Check if the database has documents
        db_data = rag_db.get(include=['metadatas', 'documents'])
        if not db_data or 'documents' not in db_data or not db_data['documents']:
            return jsonify({
                'status': 'empty', 
                'error': 'The policy knowledge base contains no documents'
            })
            
        # Extract document statistics
        doc_count = len(db_data['documents'])
        
        # Analyze document sources
        source_counts = {}
        document_sizes = []
        chunk_size_histogram = {
            "0-500": 0,
            "501-1000": 0, 
            "1001-1500": 0,
            "1501+": 0
        }
        
        for i, meta in enumerate(db_data.get('metadatas', [])):
            # Count by source document
            source = meta.get('source_doc', 'unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
            
            # Get document content length
            if i < len(db_data.get('documents', [])):
                doc_len = len(db_data['documents'][i])
                document_sizes.append(doc_len)
                
                # Add to size histogram
                if doc_len <= 500:
                    chunk_size_histogram["0-500"] += 1
                elif doc_len <= 1000:
                    chunk_size_histogram["501-1000"] += 1
                elif doc_len <= 1500:
                    chunk_size_histogram["1001-1500"] += 1
                else:
                    chunk_size_histogram["1501+"] += 1
        
        # Calculate document size statistics
        avg_size = sum(document_sizes) / len(document_sizes) if document_sizes else 0
        min_size = min(document_sizes) if document_sizes else 0
        max_size = max(document_sizes) if document_sizes else 0
        
        # Check for potentially problematic documents (very short chunks)
        short_chunks = [i for i, size in enumerate(document_sizes) if size < 50]
        short_chunk_details = []
        
        for i in short_chunks[:10]:  # Limit to first 10 to avoid huge responses
            if i < len(db_data.get('documents', [])) and i < len(db_data.get('metadatas', [])):
                short_chunk_details.append({
                    "index": i,
                    "size": len(db_data['documents'][i]),
                    "source": db_data['metadatas'][i].get('source_doc', 'unknown'),
                    "content": db_data['documents'][i][:100] + "..." if len(db_data['documents'][i]) > 100 else db_data['documents'][i],
                    "metadata": db_data['metadatas'][i]
                })
        
        # Return comprehensive database statistics
        return jsonify({
            'status': 'loaded',
            'document_count': doc_count,
            'embedding_type': str(type(rag_db._embedding_function)),
            'persist_dir': rag_db._persist_directory,
            'source_distribution': source_counts,
            'document_stats': {
                'avg_size': avg_size,
                'min_size': min_size,
                'max_size': max_size,
                'size_histogram': chunk_size_histogram
            },
            'health_check': {
                'short_chunks_count': len(short_chunks),
                'short_chunks_sample': short_chunk_details,
                'empty_metadata_count': sum(1 for meta in db_data.get('metadatas', []) if not meta or len(meta) == 0)
            }
        })
    except Exception as e:
        logging.error(f"[RAG_DEBUG] Exception in RAG endpoint: {e}", exc_info=True)
        logging.error(f"[RAG_DEBUG] RAG DB type: {type(rag_db)}")
        logging.error(f'[DEBUG] Error: {e}', exc_info=True)
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

# New endpoint to get details about specific document chunks
@ai_agent_bp.route('/debug-rag-document', methods=['GET'])
def debug_rag_document():
    """Endpoint to retrieve details about specific document(s) in the RAG database"""
    try:
        # Check if database is loaded
        if rag_db is None:
            return jsonify({'status': 'not_loaded', 'error': 'RAG database is not loaded'})
            
        # Get document name from query parameters
        doc_name = request.args.get('document')
        if not doc_name:
            return jsonify({'status': 'error', 'error': 'Missing document parameter'})
            
        # Retrieve all documents and filter by source_doc
        db_data = rag_db.get(include=['metadatas', 'documents'])
        if not db_data or 'documents' not in db_data or not db_data['documents']:
            return jsonify({'status': 'empty', 'error': 'The RAG database contains no documents'})
            
        # Filter chunks by document name
        matching_chunks = []
        for i, meta in enumerate(db_data.get('metadatas', [])):
            source_doc = meta.get('source_doc', 'unknown')
            
            # Check if this metadata matches our search
            if doc_name.lower() in source_doc.lower():
                # Get the corresponding document content
                if i < len(db_data.get('documents', [])):
                    chunk_content = db_data['documents'][i]
                    
                    # Add to our results
                    matching_chunks.append({
                        'index': i,
                        'metadata': meta,
                        'content': chunk_content,
                        'content_length': len(chunk_content)
                    })
        
        # Return the matching chunks
        return jsonify({
            'status': 'success',
            'document': doc_name,
            'matching_chunks': len(matching_chunks),
            'chunks': matching_chunks
        })
    except Exception as e:
        logging.error(f"[DEBUG-DOC] Error retrieving document chunks: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

# New endpoint to test RAG search with different query parameters
@ai_agent_bp.route('/test-rag-search', methods=['POST'])
def test_rag_search():
    """Endpoint to test different search parameters for RAG"""
    try:
        # Check if database is loaded
        if rag_db is None:
            return jsonify({'status': 'not_loaded', 'error': 'RAG database is not loaded'})
            
        # Get parameters from request
        data = request.get_json()
        query = data.get('query')
        k = int(data.get('k', 3))  # Number of results to return
        
        if not query:
            return jsonify({'status': 'error', 'error': 'Missing query parameter'})
            
        # Perform the similarity search
        docs = rag_db.similarity_search(query, k=k)
        
        # Format results
        results = []
        for i, doc in enumerate(docs):
            results.append({
                'index': i,
                'metadata': doc.metadata,
                'content': doc.page_content,
                'content_length': len(doc.page_content)
            })
            
        # Return search results
        return jsonify({
            'status': 'success',
            'query': query,
            'k': k,
            'results_count': len(results),
            'results': results
        })
    except Exception as e:
        logging.error(f"[TEST-SEARCH] Error performing test search: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500
@app.route('/')
def home_root():
    return render_template("index.html")

@ai_agent_bp.route('/verify-file/<filename>', methods=['GET'])
def verify_file(filename):
    global rag_db
    """Verify that a file exists on the server after upload."""
    docs_dir = app.config['POLICY_UPLOAD_FOLDER']
    file_path = os.path.join(docs_dir, filename)
    logging.info(f"[VERIFY] Checking if file exists: {file_path}")
    if os.path.exists(file_path) and os.path.isfile(file_path):
        file_size = os.path.getsize(file_path)
        last_modified = os.path.getmtime(file_path)
        
        # Always force reload RAG database for accurate verification
        in_rag = False
        chunk_count = 0
        normalized_filename = filename.strip().lower()
        
        try:
            # Force reload the RAG database
            # global rag_db (already declared at top)
            script_dir = os.path.dirname(os.path.abspath(__file__))
            persist_dir = os.path.join(script_dir, "chroma_db")
            
            # Only proceed if database exists
            if os.path.exists(persist_dir):
                # Clear existing database connection
                rag_db = None
                gc.collect()  # Force garbage collection
                
                # Reload with fresh embeddings
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                
                # Get database content
                db_data = rag_db.get()
                
                if db_data and 'metadatas' in db_data and db_data['metadatas']:
                    # Check each document for this filename (case insensitive)
                    for meta in db_data['metadatas']:
                        if meta and 'source_doc' in meta:
                            doc_name = meta['source_doc']
                            # Try both exact match and case-insensitive match
                            if doc_name == filename or doc_name.strip().lower() == normalized_filename:
                                in_rag = True
                                chunk_count += 1
                    
                    logging.info(f"[VERIFY] Found {chunk_count} chunks for document {filename} in RAG database")
                else:
                    logging.warning(f"[VERIFY] RAG database is empty or has no metadata")
            else:
                logging.warning(f"[VERIFY] RAG database directory does not exist at {persist_dir}")
        except Exception as e:
            logging.error(f"[VERIFY] Error checking RAG database: {e}", exc_info=True)
        
        logging.info(f"[VERIFY] File exists: {file_path}, size: {file_size} bytes, in RAG: {in_rag} with {chunk_count} chunks")
        return jsonify({
            'exists': True, 
            'size': file_size,
            'last_modified': last_modified,
            'path': file_path,
            'in_rag': in_rag,
            'chunk_count': chunk_count
        })
    else:
        logging.warning(f"[VERIFY] File does not exist: {file_path}")
        return jsonify({'exists': False}), 404

# Register blueprint and log routes for debugging
app.register_blueprint(ai_agent_bp)
logging.info("[INIT] Registered ai_agent blueprint at /ai-agent")

# Print all registered routes for debugging
logging.info("Registered routes:")
for rule in app.url_map.iter_rules():
    logging.info(f"Route: {rule.endpoint} - {rule.rule} - {rule.methods}")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5050, debug=True)


# Function to delete chunks for a specific document from the RAG database
def delete_document_chunks(document_name):
    global rag_db
    """
    Selectively delete chunks from a specific document without rebuilding the entire database.
    
    Args:
        document_name (str): The filename of the document whose chunks should be deleted
    
    Returns:
        tuple: (success, message, deleted_count)
    """
    try:
        global rag_db
        if rag_db is None:
            logging.warning(f"[DELETE-CHUNKS] No RAG database loaded, nothing to delete for {document_name}")
            return False, "No RAG database loaded", 0
        
        # Fix permissions on database directory before deleting
        try:
            persist_dir = os.path.join(os.getcwd(), 'ai_agent', 'chroma_db')
            
            if os.path.exists(persist_dir):
                try:
                    os.chmod(persist_dir, 0o777)
                    # Fix permissions on all files and directories
                    for root, dirs, files in os.walk(persist_dir):
                        for d in dirs:
                            try:
                                os.chmod(os.path.join(root, d), 0o777)
                            except Exception as e:
                                logging.warning(f"[DELETE-CHUNKS] Could not set permissions on directory {d}: {e}")
                        for f in files:
                            try:
                                os.chmod(os.path.join(root, f), 0o666)
                            except Exception as e:
                                logging.warning(f"[DELETE-CHUNKS] Could not set permissions on file {f}: {e}")
                except Exception as perm_err:
                    logging.warning(f"[DELETE-CHUNKS] Could not fix permissions on database directory: {perm_err}")
            
        except Exception as perm_err:
            logging.warning(f"[DELETE-CHUNKS] Error during permission fixing: {perm_err}")
        
        # Get all document IDs and metadatas
        logging.info(f"[DELETE-CHUNKS] Preparing to delete chunks for document: {document_name}")
        db_data = rag_db.get()
        
        if not db_data or 'ids' not in db_data or not db_data['ids']:
            logging.warning(f"[DELETE-CHUNKS] RAG database is empty, nothing to delete")
            return True, "Database is empty, nothing to delete", 0
            
        # Find IDs of chunks that belong to the document being deleted
        chunk_ids_to_delete = []
        normalized_doc_name = document_name.strip().lower()
        
        for i, meta in enumerate(db_data.get('metadatas', [])):
            if meta:
                source_doc = meta.get('source_doc', 'unknown')
                
                # Case-insensitive comparison for more reliable matching
                if source_doc == document_name or source_doc.strip().lower() == normalized_doc_name:
                    if i < len(db_data.get('ids', [])):
                        chunk_ids_to_delete.append(db_data['ids'][i])
                        
        # If no chunks found for this document
        if not chunk_ids_to_delete:
            logging.warning(f"[DELETE-CHUNKS] No chunks found for document: {document_name}")
            return True, f"No chunks found for document {document_name}", 0
            
        # Delete the chunks from the vector database
        logging.info(f"[DELETE-CHUNKS] Deleting {len(chunk_ids_to_delete)} chunks for document: {document_name}")
        
        try:
            # Now try to delete the chunks
            rag_db.delete(ids=chunk_ids_to_delete)
            
            # Persist changes to disk
            try:
                rag_db.persist()
                logging.info(f"[DELETE-CHUNKS] Successfully persisted changes after deleting {len(chunk_ids_to_delete)} chunks")
            except Exception as persist_err:
                logging.warning(f"[DELETE-CHUNKS] Persist warning (can be ignored for newer Chroma versions): {persist_err}")
            
            # Force reload database to ensure changes are applied
            try:
                global rag_db
                script_dir = os.path.dirname(os.path.abspath(__file__))
                persist_dir = os.path.join(script_dir, "chroma_db")
                
                # Get OpenAI embeddings
                openai_key = os.getenv("OPENAI_API_KEY")
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                
                # Clear existing reference
                rag_db = None
                gc.collect()  # Force garbage collection
                
                # Reload the database
                rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                logging.info(f"[DELETE-CHUNKS] Successfully reloaded database after deletion")
            except Exception as reload_err:
                logging.warning(f"[DELETE-CHUNKS] Warning reloading database after deletion: {reload_err}")
                
            logging.info(f"[DELETE-CHUNKS] Successfully deleted {len(chunk_ids_to_delete)} chunks for document: {document_name}")
            
            return True, f"Successfully deleted {len(chunk_ids_to_delete)} chunks", len(chunk_ids_to_delete)
        except Exception as delete_err:
            logging.error(f"[DELETE-CHUNKS] Error during chunk deletion: {delete_err}", exc_info=True)
            
            # If deletion fails, try a more aggressive approach - reload the database
            try:
                logging.info("[DELETE-CHUNKS] Attempting recovery by reloading the database...")
                
                # Get OpenAI embeddings
                openai_key = os.getenv("OPENAI_API_KEY")
                if not openai_key:
                    return False, "OPENAI_API_KEY is not set in the environment for recovery", 0
                
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                
                # Close existing connection
                try:
                    rag_db = None
                    gc.collect()  # Force garbage collection
                except:
                    pass
                
                # Try to reload the database
                try:
                    script_dir = os.path.dirname(os.path.abspath(__file__))
                    persist_dir = os.path.join(script_dir, "chroma_db")
                    
                    # Apply more aggressive permissions
                    try:
                        # Change directory permissions to ensure write access
                        subprocess.run(['chmod', '-R', '777', persist_dir], 
                                      check=False, capture_output=True, text=True)
                        logging.info(f"[DELETE-CHUNKS] Applied chmod 777 to {persist_dir}")
                    except Exception as chmod_err:
                        logging.warning(f"[DELETE-CHUNKS] Error changing permissions: {chmod_err}")
                    
                    # Reload database
                    rag_db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
                    
                    # Try deletion again with fresh database connection
                    rag_db.delete(ids=chunk_ids_to_delete)
                    
                    # Persist changes
                    try:
                        rag_db.persist()
                    except Exception as persist_err:
                        logging.warning(f"[DELETE-CHUNKS] Persist warning in recovery (can be ignored): {persist_err}")
                    
                    logging.info(f"[DELETE-CHUNKS] Successfully deleted {len(chunk_ids_to_delete)} chunks after recovery")
                    return True, f"Successfully deleted {len(chunk_ids_to_delete)} chunks after recovery", len(chunk_ids_to_delete)
                except Exception as reload_err:
                    logging.error(f"[DELETE-CHUNKS] Recovery attempt failed: {reload_err}", exc_info=True)
                    
                    # Final fallback: try to rebuild the database entirely without the deleted document
                    try:
                        logging.info("[DELETE-CHUNKS] Attempting recovery by rebuilding the database...")
                        import subprocess
                        script_path = os.path.join(os.path.dirname(__file__), 'ingest_docs.py')
                        
                        # Run ingestion to rebuild database
                        result = subprocess.run(['python', script_path], 
                                               capture_output=True, text=True, check=False)
                        
                        if result.returncode == 0:
                            logging.info(f"[DELETE-CHUNKS] Successfully rebuilt database")
                            return True, f"Successfully rebuilt database without {document_name}", len(chunk_ids_to_delete)
                        else:
                            logging.error(f"[DELETE-CHUNKS] Failed to rebuild database: {result.stderr}")
                            return False, f"Error deleting chunks and failed to rebuild database", 0
                    except Exception as rebuild_err:
                        logging.error(f"[DELETE-CHUNKS] Database rebuild failed: {rebuild_err}", exc_info=True)
                        return False, f"Error deleting chunks: {str(delete_err)}. Recovery failed: {str(rebuild_err)}", 0
            except Exception as recovery_err:
                logging.error(f"[DELETE-CHUNKS] Error in recovery process: {recovery_err}", exc_info=True)
                return False, f"Error deleting chunks: {str(delete_err)}. Recovery failed: {str(recovery_err)}", 0
    except Exception as e:
        logging.error(f"[DELETE-CHUNKS] Error deleting chunks for {document_name}: {e}", exc_info=True)
        return False, f"Error deleting chunks: {str(e)}", 0
