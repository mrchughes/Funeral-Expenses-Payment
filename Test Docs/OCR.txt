# OCR Implementation Guide for FEP_Local

This document provides detailed instructions for implementing OCR (Optical Character Recognition) functionality in the FEP_Local project, based on our previous work. These instructions can be used to cleanly implement OCR in a new branch without the testing issues encountered previously.

## 1. Required Dependencies

### Python Packages
Add the following packages to `python-app/app/requirements.txt`:

```
# Core OCR functionality
pytesseract
pdf2image
pillow
python-docx
pypdf
pypdf2
docx2txt

# For document processing and AI integration
langchain
langchain-community
langchain-core
langchain-openai
langchain-huggingface
openai
chromadb
sentence-transformers
transformers
```

### System Dependencies
Update the `python-app/app/Dockerfile` to include the necessary system dependencies:

```dockerfile
FROM python:3.11-slim
WORKDIR /app

# Install build dependencies and other required packages
RUN apt-get update && apt-get install -y \
build-essential \
gcc \
g++ \
python3-dev \
tesseract-ocr \
libtesseract-dev \
poppler-utils \
&& rm -rf /var/lib/apt/lists/*

# First install psutil separately (pre-built wheel)
RUN pip install --no-cache-dir psutil==5.9.8 --prefer-binary

# Copy and install requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Expose port and set the command
EXPOSE 80
CMD ["gunicorn", "-b", "0.0.0.0:80", "app:app"]
```

## 2. OCR Core Implementation

### Create the OCR Utility Module
Create a file at `python-app/app/ai_agent/ocr_utils.py`:

```python
import os
import pytesseract
import pdf2image
import io
from PIL import Image
import tempfile
from pypdf import PdfReader
import docx2txt
import re

# OCR Configuration
OCR_CONFIG = {
'lang': 'eng', # Language setting - can be expanded for multiple languages
'config': '--psm 6 --oem 3', # Page segmentation mode and OCR Engine mode
'dpi': 300 # DPI for PDF conversion
}

# Tesseract tuning parameters
custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,.;:?!@#$%^&*()-_+=<>[]{}|/\\ " -c tessedit_pageseg_mode=6'

def preprocess_image(image):
"""
Preprocess the image to improve OCR accuracy
"""
# Convert to grayscale
if image.mode != 'L':
image = image.convert('L')
# Enhance contrast using adaptive thresholding or other methods
# This is a simple threshold, but more advanced methods can be used
# image = image.point(lambda x: 0 if x < 128 else 255, '1')
return image

def extract_text_from_image(image_path):
"""
Extract text from an image file using OCR
"""
try:
image = Image.open(image_path)
image = preprocess_image(image)
# Apply OCR with custom configuration
text = pytesseract.image_to_string(image, lang=OCR_CONFIG['lang'], config=custom_config)
return text
except Exception as e:
print(f"Error extracting text from image: {e}")
return ""

def extract_text_from_pdf(pdf_path):
"""
Extract text from a PDF file using OCR if needed
"""
try:
extracted_text = ""
# First try to extract text directly (if PDF has text layer)
pdf_reader = PdfReader(pdf_path)
for page in pdf_reader.pages:
page_text = page.extract_text()
if page_text and len(page_text.strip()) > 50: # If substantial text is found
extracted_text += page_text + "\n\n"
# If sufficient text was extracted directly, return it
if len(extracted_text.strip()) > 100: # Threshold can be adjusted
return extracted_text
# Otherwise, use OCR on the PDF
images = pdf_image_conversion(pdf_path)
for image in images:
# Preprocess the image
processed_image = preprocess_image(image)
# Apply OCR with custom configuration
page_text = pytesseract.image_to_string(processed_image, lang=OCR_CONFIG['lang'], config=custom_config)
extracted_text += page_text + "\n\n"
return extracted_text
except Exception as e:
print(f"Error extracting text from PDF: {e}")
return ""

def pdf_image_conversion(pdf_path):
"""
Convert PDF to images for OCR processing
"""
try:
return pdf2image.convert_from_path(
pdf_path, 
dpi=OCR_CONFIG['dpi'],
output_folder=tempfile.gettempdir(),
fmt='png'
)
except Exception as e:
print(f"Error converting PDF to images: {e}")
return []

def extract_text_from_docx(docx_path):
"""
Extract text from a DOCX file
"""
try:
text = docx2txt.process(docx_path)
return text
except Exception as e:
print(f"Error extracting text from DOCX: {e}")
return ""

def process_document(file_path):
"""
Process a document file based on its extension
"""
_, ext = os.path.splitext(file_path)
ext = ext.lower()
if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']:
return extract_text_from_image(file_path)
elif ext == '.pdf':
return extract_text_from_pdf(file_path)
elif ext in ['.docx', '.doc']:
return extract_text_from_docx(file_path)
else:
return f"Unsupported file format: {ext}"

def clean_extracted_text(text):
"""
Clean and normalize extracted text
"""
if not text:
return ""
# Remove excessive whitespace
text = re.sub(r'\s+', ' ', text)
# Remove unusual characters and normalize
text = re.sub(r'[^\x00-\x7F]+', ' ', text)
# Other cleaning steps as needed
return text.strip()

def extract_document_metadata(file_path):
"""
Extract metadata from document if available
"""
metadata = {
"filename": os.path.basename(file_path),
"file_size": os.path.getsize(file_path),
"file_type": os.path.splitext(file_path)[1].lower(),
}
# Add more metadata extraction based on file type
return metadata
```

### Create Document Processing Service
Create a file at `python-app/app/ai_agent/document_processor.py`:

```python
import os
import json
import tempfile
from werkzeug.utils import secure_filename
from .ocr_utils import process_document, clean_extracted_text, extract_document_metadata

class DocumentProcessor:
def __init__(self, upload_folder=None):
self.upload_folder = upload_folder or os.path.join(tempfile.gettempdir(), 'uploads')
os.makedirs(self.upload_folder, exist_ok=True)
def save_uploaded_file(self, file):
"""
Save an uploaded file to disk and return the file path
"""
if not file:
return None
filename = secure_filename(file.filename)
file_path = os.path.join(self.upload_folder, filename)
file.save(file_path)
return file_path
def process_file(self, file_path):
"""
Process a document file and extract its content
"""
if not os.path.exists(file_path):
return {"error": "File not found"}
try:
# Extract text from document
raw_text = process_document(file_path)
# Clean and normalize text
cleaned_text = clean_extracted_text(raw_text)
# Extract metadata
metadata = extract_document_metadata(file_path)
return {
"success": True,
"metadata": metadata,
"text": cleaned_text,
"text_length": len(cleaned_text)
}
except Exception as e:
return {
"success": False,
"error": str(e)
}
def batch_process_files(self, file_paths):
"""
Process multiple document files
"""
results = {}
for file_path in file_paths:
if os.path.exists(file_path):
results[os.path.basename(file_path)] = self.process_file(file_path)
else:
results[os.path.basename(file_path)] = {"error": "File not found"}
return results
```

## 3. Flask API Integration

### Create OCR API Endpoint
Add a new route in `python-app/app/app.py` or create a dedicated API file:

```python
from flask import Flask, request, jsonify
from ai_agent.document_processor import DocumentProcessor
import os

app = Flask(__name__)
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

document_processor = DocumentProcessor(upload_folder=UPLOAD_FOLDER)

@app.route('/api/ocr/process', methods=['POST'])
def process_document():
if 'file' not in request.files:
return jsonify({"error": "No file part"}), 400
file = request.files['file']
if file.filename == '':
return jsonify({"error": "No selected file"}), 400
# Save and process the file
file_path = document_processor.save_uploaded_file(file)
if not file_path:
return jsonify({"error": "Failed to save file"}), 500
# Process the document
result = document_processor.process_file(file_path)
# Clean up the file after processing if needed
# os.remove(file_path)
return jsonify(result)

@app.route('/api/ocr/batch', methods=['POST'])
def batch_process_documents():
if 'files' not in request.files:
return jsonify({"error": "No files part"}), 400
files = request.files.getlist('files')
if not files or files[0].filename == '':
return jsonify({"error": "No selected files"}), 400
file_paths = []
for file in files:
file_path = document_processor.save_uploaded_file(file)
if file_path:
file_paths.append(file_path)
# Process all documents
results = document_processor.batch_process_files(file_paths)
# Clean up files after processing if needed
# for file_path in file_paths:
# os.remove(file_path)
return jsonify(results)
```

## 4. AI Integration with LangChain

### Create Document Processor with AI Integration
Create a file at `python-app/app/ai_agent/ai_document_processor.py`:

```python
import os
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from .document_processor import DocumentProcessor

class AIDocumentProcessor:
def __init__(self, model_name="gpt-3.5-turbo", embedding_model="all-MiniLM-L6-v2"):
self.document_processor = DocumentProcessor()
# Initialize embedding model
self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
# Initialize LLM
self.llm = ChatOpenAI(
model_name=model_name,
temperature=0.1,
)
# Text splitter for chunking documents
self.text_splitter = RecursiveCharacterTextSplitter(
chunk_size=1000,
chunk_overlap=200,
)
def _create_documents_from_text(self, text, metadata=None):
"""
Create LangChain Document objects from text
"""
if not metadata:
metadata = {}
# Split text into chunks
texts = self.text_splitter.split_text(text)
# Create Document objects
documents = [Document(page_content=t, metadata=metadata) for t in texts]
return documents
def process_and_analyze_document(self, file_path, queries=None):
"""
Process a document and run analysis with LLM
"""
# Process document to extract text
result = self.document_processor.process_file(file_path)
if not result.get("success", False):
return result
# Create documents from extracted text
documents = self._create_documents_from_text(
result["text"], 
metadata=result["metadata"]
)
# Create vector store
vectorstore = Chroma.from_documents(
documents=documents,
embedding=self.embeddings,
persist_directory=None # In-memory
)
# Create retriever
retriever = vectorstore.as_retriever(
search_kwargs={"k": 5}
)
# Run default analysis if no specific queries
if not queries:
queries = [
"What is the main subject of this document?",
"Summarize this document in 3-5 bullet points.",
"What are the key dates mentioned in this document?",
"Extract any financial amounts or numbers from this document."
]
# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
llm=self.llm,
chain_type="stuff",
retriever=retriever,
return_source_documents=True
)
# Run queries
analysis_results = {}
for query in queries:
try:
qa_result = qa_chain({"query": query})
analysis_results[query] = qa_result["result"]
except Exception as e:
analysis_results[query] = f"Error: {str(e)}"
# Combine results
result["analysis"] = analysis_results
return result
def extract_specific_information(self, file_path, extraction_template):
"""
Extract specific structured information from a document using a template
"""
# Process document to extract text
result = self.document_processor.process_file(file_path)
if not result.get("success", False):
return result
# Create prompt template for extraction
prompt = PromptTemplate.from_template(
"""
You are an expert document analyzer. Extract the following information from the document text.
If a piece of information is not present, return "Not found" for that field.
Document text:
{text}
Information to extract:
{extraction_template}
Return the extracted information in JSON format with the requested fields.
"""
)
# Create extraction chain
extraction_chain = (
prompt 
| self.llm 
| StrOutputParser()
)
# Run extraction
try:
extraction_result = extraction_chain.invoke({
"text": result["text"],
"extraction_template": extraction_template
})
result["extracted_info"] = extraction_result
except Exception as e:
result["extracted_info"] = {"error": str(e)}
return result
```

## 5. OCR Tuning & Performance Improvements

### Fine-tuning Tesseract OCR
To improve OCR accuracy, make the following adjustments:

1. **Page Segmentation Mode (PSM)**: Use `--psm 6` for most document pages. This treats the page as a single block of text. For complex layouts, you might need to experiment with different PSM modes.

2. **OCR Engine Mode (OEM)**: Use `--oem 3` which uses the LSTM neural network mode for better accuracy.

3. **Language Training Data**: Make sure the appropriate language data is installed. For English, use `eng`.

4. **Custom Configuration**: Add custom parameters to improve recognition, such as:
```
-c tessedit_char_whitelist="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,.;:?!@#$%^&*()-_+=<>[]{}|/\\ "
```

5. **Image Preprocessing**: Implement preprocessing steps:
- Convert to grayscale
- Apply adaptive thresholding
- Increase DPI for PDF conversion (300 DPI minimum)
- Consider denoising for scanned documents

### Post-Processing Text
Implement text post-processing to clean up the OCR results:

1. **Regex Cleanup**: Remove excessive whitespace, normalize characters, etc.
2. **Spell Checking**: Consider adding basic spell checking for common OCR errors
3. **Text Normalization**: Normalize dates, numbers, and other formatted data

## 6. Implementation Steps

1. **Add Dependencies**: Update the Dockerfile and requirements.txt as specified above
2. **Create Core OCR Module**: Implement the ocr_utils.py file
3. **Create Document Processor**: Implement the document_processor.py file
4. **Add API Endpoints**: Add the OCR API endpoints to your Flask application
5. **Implement AI Integration**: Add the AI document processor for advanced analysis
6. **Configure OCR Parameters**: Fine-tune Tesseract parameters for your specific documents
7. **Test and Validate**: Test with different document types and validate the results

## 7. Testing and Verification

To validate the OCR implementation, test with the following document types:

1. **PDF Documents**:
- PDFs with text layers
- Scanned PDFs without text layers
- PDFs with complex layouts

2. **Images**:
- High-quality scanned documents
- Photos of documents taken with cameras
- Different image formats (JPEG, PNG, TIFF)

3. **Microsoft Office Documents**:
- Word documents (.docx)
- Excel spreadsheets (for future implementation)

## 8. Future Enhancements

Consider these future enhancements for the OCR system:

1. **Table Extraction**: Add specific functionality for extracting tables from documents
2. **Form Recognition**: Implement form field recognition for structured documents
3. **Multiple Language Support**: Add support for documents in multiple languages
4. **Handwriting Recognition**: Improve recognition of handwritten text
5. **Integration with AI Models**: Deeper integration with language models for document understanding

---

This implementation provides a robust foundation for OCR functionality in the FEP_Local project. The modular design allows for easy maintenance and future enhancements.

